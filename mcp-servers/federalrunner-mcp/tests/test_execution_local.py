"""
Local execution tests for FederalRunner MCP Tools.

Tests the three MCP tools that will be exposed via HTTP endpoint:
1. federalrunner_list_wizards() - List available wizards
2. federalrunner_get_wizard_info() - Get wizard schema (THE CONTRACT)
3. federalrunner_execute_wizard() - Execute wizard with user data

Two-phase testing strategy for execute_wizard:
- Phase 1: Non-headless Chromium (visual debugging) - RUN THIS FIRST
- Phase 2: Headless WebKit (production validation) - RUN AFTER Phase 1 passes

These tests validate LOCAL execution. Remote (Cloud Run) tests will be added
after server.py is implemented with OAuth 2.1 authentication.

Uses proper logging instead of print statements (matches FederalScout pattern).
"""

import pytest
import asyncio
import logging
from pathlib import Path
import sys

# Add parent directory to path so we can import src as a package
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.execution_tools import (
    federalrunner_list_wizards,
    federalrunner_get_wizard_info,
    federalrunner_execute_wizard
)

# Test logger
logger = logging.getLogger('federalrunner.test')


# ============================================================================
# TEST DATA - Valid FSA User Data (matches fsa-estimator-schema.json)
# ============================================================================

FSA_TEST_DATA = {
    # Page 1: Student Information
    "birth_month": "05",
    "birth_day": "15",
    "birth_year": "2007",
    "marital_status": "unmarried",
    "state": "Illinois",
    "grade_level": "freshman",

    # Page 2: Student Personal Circumstances
    "has_dependents": "no",
    "personal_circumstances_none": True,

    # Page 3: Parent Marital Status
    "parents_married": "yes",

    # Page 4: Parent Information
    "parent_marital_status": "married",
    "parent_state": "Illinois",

    # Page 5: Family Size
    "family_size": 4,

    # Page 6: Parent Income and Assets
    "parent_filed_taxes": "yes",
    "parent_income": "85000",
    "parent_assets": "50000",
    "parent_child_support": "0",

    # Page 7: Student Income and Assets
    "student_filed_taxes": "no"
}

# ============================================================================
# TEST DATA - Valid Loan Simulator User Data (matches loan-simulator-borrow-more-schema.json)
# ============================================================================

LOAN_SIMULATOR_TEST_DATA = {
    # Page 1: Enrollment Information
    "program_timing": "future",

    # Page 2: Program Information
    "program_type": "Bachelor's degree",
    "program_length": "4 years",
    "dependency_status": "dependent",
    "school_location": "Illinois",
    "school_name": "University of Illinois",

    # Page 3: Family Income
    "family_income": "$75,000 - $110,000",

    # Page 4: Borrowing Amount
    "borrow_amount": 30000,

    # Page 5: Expected Salary
    "expected_salary": 55000,
    "income_growth_rate": 3,

    # Page 6: Current Loans (optional - can be empty array)
    "current_loans": []
}


# ============================================================================
# MCP TOOL TESTS
# ============================================================================

@pytest.mark.asyncio
async def test_federalrunner_list_wizards():
    """
    Test MCP Tool: federalrunner_list_wizards()

    This tool will be exposed via HTTP endpoint for Claude to call.
    It lists all discovered wizards available for execution.
    """
    result = await federalrunner_list_wizards()

    # Validate response structure
    assert result['success'] is True, f"Tool failed: {result.get('error')}"
    assert 'wizards' in result, "Response missing 'wizards' field"
    assert 'count' in result, "Response missing 'count' field"
    assert result['count'] > 0, "No wizards found"

    # Validate FSA wizard is in the list
    fsa_wizard = next((w for w in result['wizards'] if w['wizard_id'] == 'fsa-estimator'), None)
    assert fsa_wizard is not None, "FSA wizard not found in list"
    assert fsa_wizard['name'] == 'FSA Student Aid Estimator'
    assert fsa_wizard['total_pages'] == 7
    assert 'discovered_at' in fsa_wizard

    # Validate Loan Simulator wizard is in the list
    loan_wizard = next((w for w in result['wizards'] if w['wizard_id'] == 'loan-simulator-borrow-more'), None)
    assert loan_wizard is not None, "Loan Simulator wizard not found in list"
    assert 'Loan Simulator' in loan_wizard['name']
    assert loan_wizard['total_pages'] == 6
    assert 'discovered_at' in loan_wizard

    logger.info(f"‚úÖ PASSED: list_wizards found {result['count']} wizard(s)")
    logger.info(f"   FSA Wizard: {fsa_wizard['name']} ({fsa_wizard['total_pages']} pages)")
    logger.info(f"   Loan Simulator: {loan_wizard['name']} ({loan_wizard['total_pages']} pages)")


@pytest.mark.asyncio
async def test_federalrunner_get_wizard_info():
    """
    Test MCP Tool: federalrunner_get_wizard_info()

    This tool will be exposed via HTTP endpoint for Claude to call.
    It returns THE SCHEMA (the contract) that Claude uses to collect user data.

    This is the critical contract-first pattern - Claude reads the schema
    to understand what data to collect from users.
    """
    result = await federalrunner_get_wizard_info("fsa-estimator")

    # Validate response structure
    assert result['success'] is True, f"Tool failed: {result.get('error')}"
    assert result['wizard_id'] == 'fsa-estimator'
    assert 'schema' in result, "Response missing 'schema' field"

    # Validate schema structure (JSON Schema draft-07)
    schema = result['schema']
    assert '$schema' in schema, "Schema missing $schema field"
    assert 'properties' in schema, "Schema missing properties"
    assert 'required' in schema, "Schema missing required fields"

    # Validate schema has Claude hints
    assert '_claude_hints' in schema, "Schema missing Claude hints"
    assert '_example_user_data' in schema, "Schema missing example data"

    # Validate key field_ids are in schema
    assert 'birth_month' in schema['properties']
    assert 'marital_status' in schema['properties']
    assert 'state' in schema['properties']

    logger.info(f"‚úÖ PASSED: get_wizard_info returned schema for {result['wizard_id']}")
    logger.info(f"   Schema properties: {len(schema['properties'])}")
    logger.info(f"   Required fields: {len(schema['required'])}")


@pytest.mark.asyncio
@pytest.mark.slow
async def test_federalrunner_execute_wizard_non_headless(test_config):
    """
    Test MCP Tool: federalrunner_execute_wizard() [NON-HEADLESS MODE]

    This is THE MAIN TOOL - what Claude actually calls to execute wizards.

    Non-headless Chromium execution (VISUAL DEBUGGING)
    - Watch the browser fill out the FSA form step by step
    - Perfect for debugging and verifying field interactions

    This test uses configuration from .env file:
      FEDERALRUNNER_BROWSER_TYPE=chromium (recommended for visual debugging)
      FEDERALRUNNER_HEADLESS=false (shows browser window)
      FEDERALRUNNER_SLOW_MO=500 (slows down actions to watch)

    Tests the complete contract-first workflow:
    1. Load User Data Schema from wizards/data-schemas/
    2. Validate user_data against schema
    3. Load Wizard Structure from wizards/wizard-structures/
    4. Map field_id ‚Üí selector (THE CRITICAL MAPPING)
    5. Execute atomically with Playwright

    Screenshots are saved to: tests/test_output/screenshots/
    """
    logger.info("\n" + "="*70)
    logger.info("üîµ Non-Headless Chromium Execution (Visual Debugging)")
    logger.info("   Watch the browser execute the FSA wizard visually")
    logger.info("   Configuration loaded from .env file")
    logger.info(f"   Screenshots will be saved to: {test_config.screenshot_dir}")
    logger.info("="*70 + "\n")

    # Execute wizard using the MCP tool (what Claude calls!)
    # Config loads from .env file automatically
    result = await federalrunner_execute_wizard(
        wizard_id="fsa-estimator",
        user_data=FSA_TEST_DATA
    )

    # Validate response
    assert result['success'] is True, f"Execution failed: {result.get('error')}"
    assert result['wizard_id'] == 'fsa-estimator'
    assert result['pages_completed'] == 7, f"Expected 7 pages, got {result['pages_completed']}"
    assert len(result['screenshots']) > 0, "No screenshots captured"
    assert result['execution_time_ms'] > 0

    logger.info("\n" + "="*70)
    logger.info(f"‚úÖ NON-HEADLESS TEST PASSED")
    logger.info(f"   Wizard: {result['wizard_id']}")
    logger.info(f"   Execution time: {result['execution_time_ms']}ms")
    logger.info(f"   Pages completed: {result['pages_completed']}/7")
    logger.info(f"   Screenshots: {len(result['screenshots'])}")
    logger.info("="*70 + "\n")


@pytest.mark.asyncio
@pytest.mark.slow
async def test_federalrunner_execute_wizard_headless(test_config):
    """
    Test MCP Tool: federalrunner_execute_wizard() [HEADLESS MODE]

    Headless WebKit execution (PRODUCTION MODE)
    - No visible browser window
    - WebKit browser (FSA-compatible in headless mode)
    - Validates production configuration

    This test explicitly overrides configuration to run in headless mode,
    regardless of .env settings. This validates the configuration that will
    be used in Cloud Run deployment.

    Screenshots are saved to: tests/test_output/screenshots/
    """
    logger.info("\n" + "="*70)
    logger.info("üåê Headless WebKit Execution (Production)")
    logger.info("   Testing production-ready headless execution")
    logger.info(f"   Screenshots will be saved to: {test_config.screenshot_dir}")
    logger.info("="*70 + "\n")

    # Override configuration for headless execution
    import os
    original_browser = os.environ.get('FEDERALRUNNER_BROWSER_TYPE')
    original_headless = os.environ.get('FEDERALRUNNER_HEADLESS')
    original_slow_mo = os.environ.get('FEDERALRUNNER_SLOW_MO')

    try:
        # Set headless configuration
        os.environ['FEDERALRUNNER_BROWSER_TYPE'] = 'webkit'
        os.environ['FEDERALRUNNER_HEADLESS'] = 'true'
        os.environ['FEDERALRUNNER_SLOW_MO'] = '0'

        # Reload config to pick up new environment variables
        from src.config import reload_config, get_config
        new_config = reload_config()

        # Override screenshot_dir to use test output directory (same as conftest.py)
        new_config.screenshot_dir = test_config.screenshot_dir
        new_config.log_dir = test_config.log_dir

        # Execute wizard
        result = await federalrunner_execute_wizard(
            wizard_id="fsa-estimator",
            user_data=FSA_TEST_DATA
        )
    finally:
        # Restore original environment variables
        if original_browser is not None:
            os.environ['FEDERALRUNNER_BROWSER_TYPE'] = original_browser
        elif 'FEDERALRUNNER_BROWSER_TYPE' in os.environ:
            del os.environ['FEDERALRUNNER_BROWSER_TYPE']

        if original_headless is not None:
            os.environ['FEDERALRUNNER_HEADLESS'] = original_headless
        elif 'FEDERALRUNNER_HEADLESS' in os.environ:
            del os.environ['FEDERALRUNNER_HEADLESS']

        if original_slow_mo is not None:
            os.environ['FEDERALRUNNER_SLOW_MO'] = original_slow_mo
        elif 'FEDERALRUNNER_SLOW_MO' in os.environ:
            del os.environ['FEDERALRUNNER_SLOW_MO']

        # Reload config to restore original settings
        reload_config()

    # Validate response
    assert result['success'] is True, f"Execution failed: {result.get('error')}"
    assert result['wizard_id'] == 'fsa-estimator'
    assert result['pages_completed'] == 7, f"Expected 7 pages, got {result['pages_completed']}"

    logger.info("\n" + "="*70)
    logger.info(f"‚úÖ HEADLESS TEST PASSED")
    logger.info(f"   Wizard: {result['wizard_id']}")
    logger.info(f"   Execution time: {result['execution_time_ms']}ms")
    logger.info(f"   Pages completed: {result['pages_completed']}/7")
    logger.info(f"   Screenshots: {len(result['screenshots'])}")
    logger.info("="*70 + "\n")


@pytest.mark.asyncio
@pytest.mark.slow
async def test_loan_simulator_execute_wizard_non_headless(test_config):
    """
    Test MCP Tool: federalrunner_execute_wizard() for Loan Simulator [NON-HEADLESS MODE]

    Tests the Loan Simulator "Borrow More" wizard execution.
    This wizard tests:
    - Unicode dropdown handling (Bachelor's degree with smart quote)
    - Optional fields (school_location, school_name)
    - Array fields (current_loans - can be empty)
    - Number fields (borrow_amount, expected_salary, income_growth_rate)
    - Enum dropdowns (program_type, program_length, dependency_status, family_income)

    Non-headless Chromium execution (VISUAL DEBUGGING)
    - Watch the browser fill out the Loan Simulator form step by step
    - Perfect for debugging and verifying the Unicode fix works

    This test uses configuration from .env file:
      FEDERALRUNNER_BROWSER_TYPE=chromium (recommended for visual debugging)
      FEDERALRUNNER_HEADLESS=false (shows browser window)
      FEDERALRUNNER_SLOW_MO=500 (slows down actions to watch)

    Wizard structure:
    - Total pages: 6
    - Tests Unicode handling in dropdown options (Bachelor's degree)
    - Tests optional typeahead fields (school location/name)
    - Tests array fields (current loans - empty array in this test)

    Screenshots are saved to: tests/test_output/screenshots/
    """
    logger.info("\n" + "="*70)
    logger.info("üîµ Loan Simulator - Non-Headless Chromium Execution (Visual Debugging)")
    logger.info("   Watch the browser execute the Loan Simulator wizard visually")
    logger.info("   Testing: Unicode dropdowns, optional fields, arrays")
    logger.info(f"   Screenshots will be saved to: {test_config.screenshot_dir}")
    logger.info("="*70 + "\n")

    # Execute wizard using the MCP tool (what Claude calls!)
    # Config loads from .env file automatically
    result = await federalrunner_execute_wizard(
        wizard_id="loan-simulator-borrow-more",
        user_data=LOAN_SIMULATOR_TEST_DATA
    )

    # Validate response
    assert result['success'] is True, f"Execution failed: {result.get('error')}"
    assert result['wizard_id'] == 'loan-simulator-borrow-more'
    assert result['pages_completed'] == 6, f"Expected 6 pages, got {result['pages_completed']}"
    assert len(result['screenshots']) > 0, "No screenshots captured"
    assert result['execution_time_ms'] > 0

    logger.info("\n" + "="*70)
    logger.info(f"‚úÖ LOAN SIMULATOR NON-HEADLESS TEST PASSED")
    logger.info(f"   Wizard: {result['wizard_id']}")
    logger.info(f"   Execution time: {result['execution_time_ms']}ms")
    logger.info(f"   Pages completed: {result['pages_completed']}/6")
    logger.info(f"   Screenshots: {len(result['screenshots'])}")
    logger.info(f"   Unicode handling: ‚úÖ Bachelor's degree selected successfully")
    logger.info("="*70 + "\n")


# ============================================================================
# ERROR HANDLING TESTS
# ============================================================================

@pytest.mark.asyncio
async def test_execute_wizard_validation_failure():
    """
    Test that invalid user_data is caught before execution.

    The schema validator should reject invalid data and return helpful errors
    to guide Claude in collecting the correct data.

    Tests both missing fields and invalid field values.
    """
    # Test 1: Missing required fields
    invalid_data_missing = {
        "birth_month": "05",
        "birth_year": "2007"
        # Missing birth_day and other required fields
    }

    result = await federalrunner_execute_wizard(
        wizard_id="fsa-estimator",
        user_data=invalid_data_missing
    )

    assert result['success'] is False
    assert 'validation_errors' in result
    assert result['error'] == 'User data validation failed'
    assert len(result['validation_errors']['missing_fields']) > 0

    logger.info("‚úÖ PASSED: Missing fields validation caught")
    logger.info(f"   Missing fields: {len(result['validation_errors']['missing_fields'])}")

    # Test 2: Invalid field values (all required fields present but some invalid)
    invalid_data_pattern = {
        **FSA_TEST_DATA,  # Start with valid data
        "birth_month": "13",  # Invalid - doesn't match pattern ^(0[1-9]|1[0-2])$
        "birth_day": "32"     # Invalid - doesn't match pattern ^(0[1-9]|[12][0-9]|3[01])$
    }

    result = await federalrunner_execute_wizard(
        wizard_id="fsa-estimator",
        user_data=invalid_data_pattern
    )

    assert result['success'] is False
    assert 'validation_errors' in result
    assert result['error'] == 'User data validation failed'
    assert len(result['validation_errors'].get('invalid_fields', [])) > 0

    logger.info("‚úÖ PASSED: Invalid field values validation caught")
    logger.info(f"   Invalid fields: {len(result['validation_errors']['invalid_fields'])}")


@pytest.mark.asyncio
async def test_execute_wizard_nonexistent_wizard():
    """
    Test error handling for non-existent wizard.

    Should return helpful error message when wizard doesn't exist.
    """
    result = await federalrunner_execute_wizard(
        wizard_id="nonexistent-wizard",
        user_data=FSA_TEST_DATA
    )

    # Should fail with helpful error
    assert result['success'] is False
    assert 'error' in result
    assert 'nonexistent-wizard' in result['error'].lower() or 'not found' in result['error'].lower()

    logger.info("‚úÖ PASSED: Non-existent wizard error handled gracefully")
    logger.info(f"   Error: {result['error']}")


@pytest.mark.asyncio
@pytest.mark.slow
async def test_execute_wizard_runtime_error_with_screenshot():
    """
    Test runtime execution error with screenshot capture.

    üéØ VISUAL VALIDATION LOOP PATTERN (see requirements/reference/mdcalc/MDCalc-Blog.md)

    This test validates the same self-correcting pattern used in the MDCalc agent:

    1. Schema validation passes ‚úÖ (type-safe contract upheld)
    2. Runtime execution fails ‚ùå (form shows validation error like "Select a response")
    3. Error screenshot captured üì∏ (visual context of failure)
    4. Claude Vision analyzes screenshot + error message
    5. Claude guides user to correct the issue (e.g., "provide valid US state")
    6. Re-execute with corrected data

    From MDCalc blog: "The agent takes another screenshot to check for validation errors.
    This creates a self-correcting loop. The agent sees errors exactly as a human would
    and adapts on the fly. No error codes to parse, no API documentation to maintain‚Äî
    just visual understanding."

    Test scenario: International student currently studying abroad
    - Parent lives in: "California" (US state - valid)
    - Student currently in: "Kerala, India" (studying abroad)
    - User provides "Kerala, India" for student state field

    Schema validation: PASSES (state field accepts any string per schema)
    Runtime execution: FAILS (FSA form rejects "Kerala, India" - requires US state)
    Visual error: Screenshot shows "Select a response" validation message

    Expected behavior:
    - success: False
    - error: Contains helpful error message about field failure
    - screenshots: Includes error screenshot showing where execution failed
    - pages_completed < 7 (execution failed before completion)
    - Claude Vision can analyze screenshot + error to:
      1. See the "Select a response" validation error visually
      2. Understand FSA requires US state for legal residence
      3. Guide user: "For dependent students, use parent's state (California)"
      4. Re-execute with corrected data

    This is the universal pattern for form automation without APIs - visual intelligence
    that self-corrects based on what it sees, just like a human would.
    """
    logger.info("\n" + "="*70)
    logger.info("üî¥ Runtime Execution Error Test (with Screenshot Capture)")
    logger.info("   Scenario: Student studying abroad in Kerala, India")
    logger.info("   Parent lives in California, USA")
    logger.info("   User incorrectly provides student's current location instead of legal residence")
    logger.info("="*70 + "\n")

    # Create test data that passes validation but fails at runtime
    # Realistic scenario: International student studying in India
    FSA_TEST_DATA_INVALID_STATE = {
        **FSA_TEST_DATA,
        "state": "Kerala, India",        # Student's CURRENT location (studying abroad)
        "parent_state": "California"     # Parent's US state (correct)
    }

    result = await federalrunner_execute_wizard(
        wizard_id="fsa-estimator",
        user_data=FSA_TEST_DATA_INVALID_STATE
    )

    # Pretty-print the full result for debugging
    import json
    logger.info("\n" + "="*70)
    logger.info("üìã Full execution result:")
    logger.info("="*70)
    # Create a copy without screenshots (too large to log)
    result_without_screenshots = {k: v for k, v in result.items() if k != 'screenshots'}
    result_without_screenshots['screenshots'] = f"<{len(result.get('screenshots', []))} screenshots captured>"
    logger.info(json.dumps(result_without_screenshots, indent=2))
    logger.info("="*70 + "\n")

    # Should fail at runtime (not validation)
    assert result['success'] is False, "Expected execution to fail with invalid state"
    assert 'error' in result, "Response missing 'error' field"
    assert 'error_type' in result, "Response missing 'error_type' field"

    # Should have captured screenshots (including error screenshot)
    assert 'screenshots' in result, "Response missing 'screenshots' field"
    assert len(result['screenshots']) > 0, "No screenshots captured (should have error screenshot)"

    # Should report which page it failed on
    assert 'pages_completed' in result
    # Should fail somewhere during execution (not complete all 7 pages)
    # Note: The actual failure may vary - typeahead might accept the value but cause issues later
    assert result['pages_completed'] < 7, f"Should fail before completing all pages, got {result['pages_completed']}"

    # Should have execution time
    assert 'execution_time_ms' in result
    assert result['execution_time_ms'] > 0

    logger.info("‚úÖ PASSED: Runtime error captured with context")
    logger.info(f"   Error type: {result['error_type']}")
    logger.info(f"   Error message: {result['error']}")
    logger.info(f"   Pages completed before error: {result['pages_completed']}")
    logger.info(f"   Screenshots captured: {len(result['screenshots'])}")
    logger.info(f"   Execution time: {result['execution_time_ms']}ms")
    logger.info("\n   üí° Claude can use this error + screenshot to:")
    logger.info("      1. Show user the error screenshot")
    logger.info("      2. Analyze what went wrong (field selector, validation, etc.)")
    logger.info("      3. Provide guidance on correcting the input")
    logger.info("      4. Re-execute with corrected data")
    logger.info("\n   üìù Note: This test validates error recovery pattern:")
    logger.info("      - Data passes schema validation ‚úÖ")
    logger.info("      - Execution encounters runtime error ‚ùå")
    logger.info("      - Error + screenshot captured for Claude to analyze üì∏")
    logger.info("="*70 + "\n")
